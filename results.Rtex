<<setup-results, echo=FALSE, message=FALSE, warning=FALSE, results='hide'>>=
source("common.R")
@

\section{Comparison of TestSuite\_AH and TestSuite\_EP}\label{sec:comparison-results}

In this section, we will answer \textbf{RQ1: \textit{How much} do the measurements obtained with TestSuite\_AH and TestSuite\_EP differ from each other?}


In terms of measurement theory, analyzing the differences between the AH and EP datasets is equivalent to assessing the accuracy (trueness and precision) of TestSuite\_AH and TestSuite\_EP. We will use the four comparison methods (repeatability, intermediate precision, Bland-Altman plot and the ICC) described in Section~\ref{sec:comparison}.

\subsection{Repeatability analysis}\label{repeatability-analysis}

Test suites are popular measuring instruments because the measurement is automatic and \textbf{repeatable}. Usually, running the same test suite on the same code always yields the same results \footnote{Varying results are possible when the code depends on some random input. For proper testing in those cases, the random portion should be isolated, e.g., using mocking, to achieve deterministic results \cite{Koskela2013}.}.

One of the sources of uncertainty when using test suites for measurement is the human operator. To perform the measurement, the operator at least should: (1) download the subject's code, (2) add TestSuite\_AH and TestSuite\_EP, (3) make the necessary adjustments to the code (e.g., resolve compilation problems), (4) run the test suites, and (5) write down the results. Measurement problems take place in the step 3. Steps 1-2 influence sample preparation (not measurement), step 4 is deterministic, and only transcription problems may take place in step 5.

Step 3 can be conducted in different ways:

\begin{itemize}

\item One option is not to make any change to the subjectsâ€™ code. In this case, due to the deterministic nature of the measurement with test suites, the obtained measures always have the same value, which implies that $s_r = 0$. 

\item In situations where the code does not compile%
\footnote{The participants' code may be fully functional, but compilation may happen due to  the connection between the participants' code and the test suites.} %
due to clerical inconsistencies, e.g., method names, parameter ordering, the experimenters can apply minimal fixes before running the test suite. These fixes are predictable because they only aim to align the participants' code and test suite abstractions. Being predictable, the fixes are also repeatable, i.e., the same measurer would apply the same fixes every time the measurement is performed, particularly when the time intervals between measurements are short. Given the definition of \textit{repeatability} (see Section~\ref{sec:repeatability}), $s_r = 0$. 

Beyond the predictable nature of the fixes, code measurement through test suites possesses a distinctive property that is not typically found in measurement processes: configurations (participants' code, fixes, and test suites) can be preserved and reproduced exactly at a later time. This is not merely a theoretical possibility; in practice, researchers conducting such measurements intuitively tend to save the modifications even before executing the test suites. In other words, the repeatability of code measurements, in general, is $s_r = 0$ always.

\item Finally, more complex strategies for connecting the participants' code and the test suites, such as adjusting loop bounds, reordering or reinvoking methods, or creating helper methods, could be applied. This does not occur in this research, but it is theoretically possible and has likely been done elsewhere.

Complex changes have implications for the repeatability of measurements. Complex changes are probably not predictable, implying that the same experimenter could apply different changes in different measuring sessions%
\footnote{Notice that when the measuring sessions are close in time, the changes applied are probably the same giving $s_r = 0$.}%
. That would mean that $s_r > 0$. However, we should keep in mind that code changes can be preserved by saving, as indicated above. Applying different changes in different measuring sessions seems more a methodological than a measurement problem. Accordingly, they should be treated as threats to the validity of the instrumentation. Thus, we must conclude again that $s_r = 0$.

\end{itemize}

\begin{framed}

In Experiment\_PT and Experiment\_EC, the measurer made small changes (e.g., method names, the order of parameters, etc.) to avoid zero Variable\_QLTY scores due to clerical errors. Using this strategy, when measurements are repeated \textbf{in a short time}, the results do not vary, because the changes are predictable. Thus, $s_r = 0$ again.

\end{framed}



\subsection{Intermediate precision}\label{intermediate-precision-analysis}

The intermediate precision $s_{R_w}$ represents the uncertainty produced by the measuring instruments. The intermediate precision assumes that the replicate measurements are made ''on the same or similar object'' (see Section~\ref{sec:intermediate}). This implies a problem in SE experiments. 
We cannot assume that the pieces of code collected in an experiment are similar; in fact, they exhibit a great degree of variation.

Providentially, the model proposed in ISO 5725-3 \cite{ISO5725} can be expanded with additional factors, as long as the nesting structure is specified in the model. We have performed the measurement on \Sexpr{nrow(expdata[expdata$Instrument=="EP",])} programs from both Experiment\_PT and Experiment\_EC \footnote{The origin of the code (Experiment\_PT or Experiment\_EC sites) is irrelevant in the accuracy of TestSuite\_AH and TestSuite\_EP}. The Measurement Instrument is nested within the new factor Program. The corresponding model is a simple extension
\footnote{This model can be seen as a restricted version of a more general procedure for the comparison of variances. See \cite[Chapter 9]{box2005statistics} for details.}
of Eq.~\ref{eq:linear-model-intermediate}:
\begin{equation}
\label{eq:linear-model-intermediate-extended}
Variable\_QLTY = Program/Instrument + \epsilon
\end{equation}

\iffalse
The analysis was conducted with the following R command:

<<intermediate-precision-analysis-1, echo=TRUE>>=
lm <- aov(QLTY ~ Program/Instrument,
          data = expdata)
@
\fi

\iffalse
<<intermediate-precision-analysis-2>>=
tmp <- tidy(lm)
Imeansq <- tmp$meansq[2]
s2M <- round(Imeansq, 2)
sM <- round(sqrt(s2M), 2)
@
\fi

\begin{table}[htb]
    \small
    \centering
    \caption{Estimation of $s_M$ using Eq.~\ref{eq:linear-model-intermediate-extended}}
    \label{tab:intermediate-precision-analysis}
    \resizebox{\columnwidth}{!}{
        <<intermediate-precision-analysis-3, results="asis">>=
        print(xtable(lm, digits = 2), floating = FALSE)
        @
    }
\end{table}

Table~\ref{tab:intermediate-precision-analysis} shows the analysis results. The residual is zero, because as we explained above $s_r = 0$. $s^2_M$ is calculated as \cite[pp. 619--624]{montgomery2017design}:
\begin{equation}
\label{eq:linear-model-intermediate-extended-component-variance}
s^2_M = MS(Program:Instrument) = \Sexpr{s2M}
\end{equation}

\iffalse
The intermediate precision of the measuring instruments is $s_M = \sqrt{s^2_M + s^2_r} = \sqrt{\Sexpr{s2M} + 0} = \Sexpr{sM}$. Using $k = 2$, the expanded uncertainty (see Eq.~\ref{eq:expanded-uncertainty2}) is $2 \times \Sexpr{sM} = \Sexpr{(eu <- 2*sM)}$.
\fi

\begin{framed}

When TestSuite\_AH and TestSuite\_EP are used as measuring instruments (e.g., in two different experiments, later combined using meta-analysis), measures that theoretically speaking should be similar (e.g., because the measured programs exhibit the same Variable\_QLTY) can differ up to $\pm \Sexpr{eu}\%$.

\end{framed}

\subsection{Bland-Altman method}

\iffalse
\begin{figure}[!t]
<<bland-altman-analysis-1, results='hide'>>=
# # extract the data required by Meth: Instrument, item measured, replication number...
# item <- do.call(paste0, expdata[c(2, 3)])
# meth <- expdata$Instrument
# repl <- rep(1, nrow(expdata))
# 
# # QLTY measurement
# y <- expdata$QLTY
# dataqlty <- Meth(data.frame(meth, item, repl, y))
# 
# # Bland-altman plots
# par(mar=c(5, 4, 4, 4) + 0.1)
# BA.plot(dataqlty,
#     main = "Quality",
#     alpha = NULL,
#     axlim=c(0,100),
#     diflim = c(-100, 100),
#     digits = 2,
#     col.lines = "black",
#     cex =  0.8,
#     las = 1,
#     pch = 1)

# calculation of differences
auxq <- expdata[c("Instrument", "Program", "QLTY")]
auxq <- reshape(auxq, idvar = "Program", timevar = "Instrument", direction = "wide")
auxq <- auxq[c("QLTY.EP", "QLTY.AH")]
dqlty <- auxq$`QLTY.EP` - auxq$`QLTY.AH`
md <- mean(dqlty)
sd <- sd(dqlty)
lower_limit <- round(md + 2 * sd, 2)
upper_limit <- round(md - 2 * sd, 2)
md <- round(md, 2)
sd <- round(sd, 2)
@
\fi

\iffalse
  \includegraphics[scale=0.5]{figures/fig-bland-altman-analysis-1-1.pdf}
  \caption{Bland-Altman plot}
  \label{fig:bland-altman-plot}
\end{figure}

\fi






The Bland-Altman method uses the differences between measures (Eq.~\ref{eq:bland-altman-difference}) to calculate the accuracy of the measuring instruments. The mean difference $\bar{d} = \Sexpr{md}$ means that the TestSuite\_AH and TestSuite\_EP differ \Sexpr{abs(md)}\% units \textit{in average} (notice that Variable\_QLTY is measured as a percentage). The standard deviation of the differences is $s_d = \Sexpr{sd}$.

\iffalse
Fig.~\ref{fig:bland-altman-plot} shows the same information visually. The central line represents the mean difference $\bar{d} = \Sexpr{md}$, whereas the top and bottom lines delimit the range of variation of the differences between measurements (the points displayed in the plot). Those limits are calculated as $\bar{d} \pm 2 \times s_d = \Sexpr{md} \pm \Sexpr{2 * sd}$.
\fi

\begin{framed}

According to the Bland-Altman method, the measures made on the same code by TestSuite\_AH and TestSuite\_EP may vary up to \Sexpr{2 * sd}\% in either direction. Actually, we can see in Fig.~\ref{fig:bland-altman-plot} some measures that even exceed such limits. TestSuite\_AH tends to give higher values (\Sexpr{abs(md)}\% in average) than TestSuite\_EP.

\end{framed}

When the measuring instruments are not biased (one does not give higher or lower measures than the other systematically, i.e., $\bar{d} = 0$), and their precision is equal, $s_d$ and $s_M$ (the intermediate precision) hold a mathematical relationship:
\begin{equation}
\label{eq:relationship-intermediate-precision-bland-altman}
s_d = \sqrt{2} \times s_M
\end{equation}

In such a case, $s_d >> s_M$. Please notice that it does not mean that the intermediate precision is ''more conservative'' than the Bland-Altman method. The Bland-Altman method deals with differences between measures and the intermediate precision with the measures themselves. As the mean difference $\bar{d}$ increases, the values of $s_d$ and $s_M$ diverge.

\subsection{ICC}\label{sec:icc-results}

The ICC is obtained using Eq.~\ref{eq:icc} which, in turn, requires the calculation of the linear model depicted in Eq.~\ref{eq:linear-model-icc}. 
\iffalse
That model is calculated using the R command:

<<icc-analysis-1, echo = TRUE>>=
lm <- aov(QLTY ~ 1 +  Instrument + Program,
          data = expdata)
@
\fi
\iffalse
<<icc-analysis-2, echo = FALSE, results = 'hide' >>=
tmp <- tidy(lm)
Cmeansq <- tmp$meansq[2]
Rmeansq <- tmp$meansq[3]
s2C <- round((Cmeansq - Rmeansq)/2, 2)
s2e <- round(Rmeansq, 2)
@
\fi
\begin{table}[htb]
\small
\centering
\caption{Estimation of $s_I$ and $\epsilon$ using Eqs.~\ref{eq:icc-component-variance-error} and \ref{eq:icc-component-variance-instrument}}
\label{tab:icc-analysis}
<<icc-analysis-3, results='asis'>>=
print(xtable(lm,
          digits = 2),
     floating = FALSE)
@
\end{table}

The results of the analysis is shown in Table~\ref{tab:icc-analysis}. The value of $s^2_C$ is (see Eqs.~\ref{eq:icc-component-variance-error} and \ref{eq:icc-component-variance-instrument}):
\begin{center}
$s^2_C = \frac{\Sexpr{round(Cmeansq, 2)} - \Sexpr{round(Rmeansq, 2)}}{\Sexpr{2}} = \Sexpr{s2C}$
\end{center}

Therefore, the ICC(3, 1) is (see Eq.~\ref{eq:icc}):
\begin{center}
$\rho = \frac{s^2_C}{s^2_C + s^2_e} = \frac{\Sexpr{s2C}}{\Sexpr{s2C} + \Sexpr{s2e}} = \Sexpr{round(s2C/(s2C+s2e), 2)}$
\end{center}

The interpretation of $\rho$ is not evident. As a general rule, the lower the value, the less related the measures taken by TestSuite\_AH and TestSuite\_EP for the same program. To interpret the ICC more easily, reference values are typically given in the literature. 

\begin{framed}

Two measurement instruments are considered to have a good agreement when $\rho \geq 0.75$ \cite{Fleiss2011}. In our case, TestSuite\_AH and TestSuite\_EP do not achieve that level.

\end{framed}

\textbf{To answer RQ1}, the statistical analyses yield substantially different results between TestSuite\_AH and TestSuite\_EP. The AH and EP datasets exhibit significant discrepancies. Specifically, TestSuite\_AH generally shows larger effects and variances than TestSuite\_EP. Differences between these test suites are so pronounced that they can lead to contradictory results, implying that the choice of test suite can substantially impact the experiment outcomes. This emphasizes the importance of careful selection and validation of test suites in software engineering experiments to ensure reliable and consistent measurement.
